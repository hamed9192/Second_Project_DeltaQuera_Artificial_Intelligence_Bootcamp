{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f35f4e",
   "metadata": {},
   "source": [
    "<p style='color:red ; font-size:32px; text-align:center;' dir=rtl>\n",
    "Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… \n",
    "<br>\n",
    "Ù‚Ø³Ù…Øª Ø³ÙˆÙ… : ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³ Ù†Ø¸Ø±Ø§Øª\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf1311",
   "metadata": {},
   "source": [
    "<p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71abc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.H.A\\anaconda3\\envs\\sentiment_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification \n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e6556",
   "metadata": {},
   "source": [
    "<p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f97b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡: cuda\n",
      "Ù†Ø§Ù… GPU: NVIDIA GeForce RTX 2060\n",
      "Ø­Ø§ÙØ¸Ù‡ GPU (Ú©Ù„ÛŒ): 6.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ ---\n",
    "MODEL_NAME = 'roberta-base' # ğŸŒŸ ØªØºÛŒÛŒØ± Ø§ØµÙ„ÛŒ: Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ù‡ roberta-base\n",
    "MAX_LEN = 128                 \n",
    "BATCH_SIZE = 16               \n",
    "NUM_EPOCHS = 3                \n",
    "LEARNING_RATE = 2e-5          \n",
    "\n",
    "# --- ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÚ¯Ø§Ù‡ (CPU ÛŒØ§ GPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Ù†Ø§Ù… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Ø­Ø§ÙØ¸Ù‡ GPU (Ú©Ù„ÛŒ): {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME) \n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5) \n",
    "model.to(device) # Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¯Ù„ Ø¨Ù‡ GPU (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)\n",
    "\n",
    "# --- Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ ---\n",
    "@dataclass\n",
    "class EncodedData:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    labels: torch.Tensor = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97380d",
   "metadata": {},
   "source": [
    "<p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "2. Ú©Ù„Ø§Ø³ Dataset Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ø¨Ø§ Ù„ÛŒØ¨Ù„) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f643e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ú©Ù„Ø§Ø³ Dataset Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ø¨Ø§ Ù„ÛŒØ¨Ù„) ---\n",
    "class TrainReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings \n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][item].flatten(),\n",
    "            'attention_mask': self.encodings['attention_mask'][item].flatten(),\n",
    "            'labels': torch.tensor(self.labels[item] - 1, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e05a84",
   "metadata": {},
   "source": [
    "<p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "3. Ú©Ù„Ø§Ø³ Dataset Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ (Ø¨Ø¯ÙˆÙ† Ù„ÛŒØ¨Ù„) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34206d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Ú©Ù„Ø§Ø³ Dataset Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (Ø¨Ø¯ÙˆÙ† Ù„ÛŒØ¨Ù„) ---\n",
    "class PredictReviewDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][item].flatten(),\n",
    "            'attention_mask': self.encodings['attention_mask'][item].flatten(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baab8e5",
   "metadata": {},
   "source": [
    "<p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "4. ØªØ§Ø¨Ø¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f753567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ØªØ§Ø¨Ø¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ ---\n",
    "def load_and_preprocess_data_for_training_and_prediction(train_file_path, predict_file_path, text_col='reviewText', target_col='overall'):\n",
    "    try:\n",
    "        temp_train_df = pd.read_csv(train_file_path)\n",
    "        \n",
    "        #  30000 Ø³Ø·Ø± ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø¢Ù† Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "        # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³Ø·Ø±Ù‡Ø§ Ú©Ù…ØªØ± Ø§Ø² 30000 Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ù…Ù‡ Ø³Ø·Ø±Ù‡Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "        train_df = temp_train_df.sample(n=min(30000, len(temp_train_df)), random_state=42) \n",
    "        predict_df = pd.read_csv(predict_file_path) \n",
    "        \n",
    "        print(f\"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² '{train_file_path}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(train_df)}\")\n",
    "        print(f\"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø² '{predict_file_path}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(predict_df)}\")\n",
    "        \n",
    "        train_df.dropna(subset=[text_col, target_col], inplace=True)\n",
    "        train_df[text_col] = train_df[text_col].astype(str)\n",
    "        train_df[target_col] = train_df[target_col].astype(int)\n",
    "        \n",
    "        predict_df.dropna(subset=[text_col], inplace=True) \n",
    "        predict_df[text_col] = predict_df[text_col].astype(str)\n",
    "        \n",
    "        return train_df, predict_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Ø®Ø·Ø§: ÛŒÚ©ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db9b7d",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯: ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ú©Ø§Ø±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f17cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯: ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ú©Ø§Ø± ---\n",
    "def tokenize_data_batch(texts, tokenizer, max_len):\n",
    "    print(\"\\nâ³ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\")\n",
    "    encodings = tokenizer.batch_encode_plus(\n",
    "        texts.tolist(), \n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    print(\"âœ… ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\")\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb3863",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    " 5. ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª DataLoaderÙ‡Ø§ (Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bece88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª DataLoaderÙ‡Ø§ (Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡) ---\n",
    "def create_data_loaders(train_df, predict_df, tokenizer, max_len, batch_size):\n",
    "    if train_df is None or predict_df is None:\n",
    "        return None, None\n",
    "\n",
    "    train_encodings = tokenize_data_batch(train_df.reviewText, tokenizer, max_len)\n",
    "    predict_encodings = tokenize_data_batch(predict_df.reviewText, tokenizer, max_len)\n",
    "\n",
    "    train_dataset = TrainReviewDataset(\n",
    "        encodings=train_encodings,\n",
    "        labels=train_df.overall.to_numpy()\n",
    "    )\n",
    "\n",
    "    predict_dataset = PredictReviewDataset(\n",
    "        encodings=predict_encodings\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0 \n",
    "    )\n",
    "\n",
    "    predict_data_loader = DataLoader(\n",
    "        predict_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0 \n",
    "    )\n",
    "    \n",
    "    print(f\"\\nØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {len(train_data_loader)}\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: {len(predict_data_loader)}\")\n",
    "    return train_data_loader, predict_data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f035f",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    " 6. ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1953cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Epoch ---\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train() \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels \n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n",
    "        optimizer.step() \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / total_samples, sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6174a",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    "7. ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (Ø¨Ø¯ÙˆÙ† Ù„ÛŒØ¨Ù„) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ccaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (Ø¨Ø¯ÙˆÙ† Ù„ÛŒØ¨Ù„) ---\n",
    "def predict_on_new_data(model, data_loader, device):\n",
    "    model.eval() \n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for d in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return [p + 1 for p in all_predictions] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c6da8",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    " 8. ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e09a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„ ---\n",
    "def run_deep_learning_sentiment_prediction(train_file_path, predict_file_path, text_col='reviewText', target_col='overall'):\n",
    "    train_df, predict_df = load_and_preprocess_data_for_training_and_prediction(train_file_path, predict_file_path, text_col=text_col, target_col=target_col)\n",
    "\n",
    "    if train_df is None or predict_df is None:\n",
    "        print(\"ğŸš¨ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
    "        return\n",
    "\n",
    "    train_data_loader, predict_data_loader = create_data_loaders(\n",
    "        train_df, predict_df, tokenizer, MAX_LEN, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"\\n--- Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "        print(f\"âœ¨ Ø¢Ù…ÙˆØ²Ø´ - Ø¯Ù‚Øª: {train_acc:.4f}, Ø®Ø·Ø§: {train_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---\")\n",
    "    \n",
    "    predicted_ratings = predict_on_new_data(model, predict_data_loader, device)\n",
    "\n",
    "    submission_df = pd.DataFrame({'predicted': predicted_ratings})\n",
    "    \n",
    "    output_file_path = 'q2_submission.csv'\n",
    "    submission_df.to_csv(output_file_path, index=False) \n",
    "\n",
    "    print(f\"\\nğŸ‰ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ '{output_file_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\")\n",
    "    print(\"ğŸ“‹ Û±Û° Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ:\")\n",
    "    print(submission_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa01ba1",
   "metadata": {},
   "source": [
    " <p style='color:yellow ; font-size:28px; text-align:center;' dir=rtl>\n",
    " Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù†Ù‡Ø§ÛŒÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51074e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\ipykernel_7420\\1440439958.py:4: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp_train_df = pd.read_csv(train_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² 'train_data.csv' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: 30000\n",
      "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø² 'test_data.csv' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: 20000\n",
      "\n",
      "â³ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\n",
      "âœ… ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\n",
      "\n",
      "â³ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\n",
      "âœ… ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\n",
      "\n",
      "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: 1875\n",
      "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: 1250\n",
      "\n",
      "--- Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---\n",
      "\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [18:04<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Ø¢Ù…ÙˆØ²Ø´ - Ø¯Ù‚Øª: 0.6781, Ø®Ø·Ø§: 0.8164\n",
      "\n",
      "--- Epoch 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [18:17<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Ø¢Ù…ÙˆØ²Ø´ - Ø¯Ù‚Øª: 0.7353, Ø®Ø·Ø§: 0.6618\n",
      "\n",
      "--- Epoch 3/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [18:02<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Ø¢Ù…ÙˆØ²Ø´ - Ø¯Ù‚Øª: 0.7785, Ø®Ø·Ø§: 0.5641\n",
      "\n",
      "--- Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [02:13<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ 'q2_submission.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\n",
      "ğŸ“‹ Û±Û° Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ:\n",
      "   predicted\n",
      "0          1\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          1\n",
      "5          1\n",
      "6          1\n",
      "7          1\n",
      "8          1\n",
      "9          2\n"
     ]
    }
   ],
   "source": [
    "# --- Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ---\n",
    "if __name__ == \"__main__\":\n",
    "    train_csv_file_path = 'train_data.csv' \n",
    "    predict_csv_file_path = 'test_data.csv' \n",
    "    \n",
    "    run_deep_learning_sentiment_prediction(\n",
    "        train_csv_file_path, \n",
    "        predict_csv_file_path, \n",
    "        text_col='reviewText', \n",
    "        target_col='overall'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e82c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± 'roberta_sentiment_model_trained' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\n"
     ]
    }
   ],
   "source": [
    "# --- Ú©Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ---\n",
    "# Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Ø¢Ù† Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "output_dir = \"roberta_sentiment_model_trained\" \n",
    "\n",
    "# Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ú©Ù‡ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ (ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± '{output_dir}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\") # ØªØºÛŒÛŒØ± Ø¨Ù‡ print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20074fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
